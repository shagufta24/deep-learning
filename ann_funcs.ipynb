{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z = np.dot(W,A) + b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b) # This \"linear_cache\" contains (A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z) # This \"activation_cache\" contains \"Z\"\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b) # This \"linear_cache\" contains (A_prev, W, b)\n",
    "        A, activation_cache = relu(Z) # This \"activation_cache\" contains \"Z\"\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    cost = (-1/m) * (np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T))\n",
    "\n",
    "    # cost = np.mean(np.power(Y-AL, 2))\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    # Here cache is \"linear_cache\" containing (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[L-1] # Last Layer\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. (≈ 1 line of code)\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000)\n",
      "(1, 1000)\n",
      "(1, 300)\n",
      "(1, 300)\n"
     ]
    }
   ],
   "source": [
    "from sine_data import *\n",
    "\n",
    "x_train, y_train, x_val, y_val = generate_sine_data()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.500078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shagu\\AppData\\Local\\Temp/ipykernel_17304/3807674404.py:25: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
      "c:\\Users\\shagu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(x_train, y_train, layers_dims, num_iterations = 10, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a70e35cc37e5d6b43581a2bbcc812898b7c7399b258b00c62cbb1ff710d022c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
